{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500c9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ferasdahrooge/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ferasdahrooge/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ferasdahrooge/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import datetime as dt\n",
    "import praw\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language = \"english\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34d94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tickers based on https://www.nasdaq.com/market-activity/stocks/screener\n",
    "\n",
    "tickers = pd.read_csv('listoftickers.csv')\n",
    "\n",
    "tickers = tickers.Symbol.to_list()\n",
    "tickers.remove(\"OC\")\n",
    "tickers.remove(\"DD\")\n",
    "tickers.remove(\"TA\")\n",
    "\n",
    "# Remove single character tickers\n",
    "tickers = [str(x) for x in tickers if len(str(x)) != 1]\n",
    "\n",
    "tickers.append(\"SPY\")\n",
    "tickers.append(\"V\") #Visa\n",
    "tickers.append(\"T\") #AT&T\n",
    "tickers.append(\"C\") #Citigroup\n",
    "\n",
    "# tickers = [str(x).lower() for x in tickers]\n",
    "\n",
    "# Remove english words from tickers? Assume tickers will be all uppercase for now\n",
    "# english_words = set(words.words('en'))\n",
    "# tickers = [x for x in tickers if x not in english_words]\n",
    "# tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'wallstreetbets'\n",
    "\n",
    "# Columns we want. Full List here https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/14-Reddit-Data.html\n",
    "filters = ['id', 'title', 'selftext', 'author', 'score', 'num_comments']\n",
    "\n",
    "# Set some maximum number of posts\n",
    "limit = 1000000\n",
    "\n",
    "# Set minimum score\n",
    "min_score = 100\n",
    "\n",
    "# Include posts between these two dates\n",
    "start_time = dt.datetime(2021, 2, 5)\n",
    "end_time = dt.datetime(2022, 11, 15)\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Pull posts one day at a time, save as pickle\n",
    "while(end_time <= dt.datetime(2021, 2, 6)):\n",
    "    api_request_generator = api.search_submissions(\n",
    "            subreddit=subreddit,   #Subreddit we want to audit\n",
    "            after=int(start_time.timestamp()),      #Start date\n",
    "            before=int(end_time.timestamp()),       #End date\n",
    "            filter=filters,        #Column names we want to retrieve\n",
    "            limit=limit)\n",
    "\n",
    "    df = pd.DataFrame([submission.d_ for submission in api_request_generator])\n",
    "\n",
    "    # Convert timestamp format to datetime\n",
    "    df['Time of Creation'] = df['created_utc'].map(lambda t: dt.datetime.fromtimestamp(t))\n",
    "    df = df.drop(['created_utc', 'created'], axis=1)\n",
    "\n",
    "    df.to_pickle(\"./pickles/old score by day/{}.pkl\".format(\n",
    "        str(start_time.year) + '-' + str(start_time.month) + '-' + str(start_time.day)))\n",
    "    \n",
    "    start_time += dt.timedelta(days=1)\n",
    "    end_time += dt.timedelta(days=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28338562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushshift does not update the score\n",
    "# Link to Reddit's API which we can use to get updated score based on the post ID\n",
    "\n",
    "client_id = 'inJ6zJcKI7dSDOEnYtZtSg'\n",
    "client_secret = 'HMVgC-yLRwpqsWAnYfPulFCna2ZbRA'\n",
    "user_agent = 'ENSF612 - Pulling Score'\n",
    "password = 'mengsoftware'\n",
    "username = 'ensf612'\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.datetime(2021, 2, 7)\n",
    "min_score = 100\n",
    "\n",
    "# Import raw pickle by day, fix the score, \n",
    "while (time <= dt.datetime(2021, 5, 1)):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(\"./pickles/old score by day/{}.pkl\".format(\n",
    "            str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "    df['new_id'] = \"t3_\" + df[\"id\"]\n",
    "\n",
    "                        \n",
    "    gentest = reddit.info(fullnames = df['new_id'].to_list())\n",
    "    df['score'] = pd.Series([item.score for item in gentest])\n",
    "    df = df.dropna(subset=['score'])\n",
    "    \n",
    "    df = df[df['score'] > min_score]\n",
    "                        \n",
    "    df.to_pickle(\"./pickles/new score by day/{}.pkl\".format(\n",
    "        str(time.year) + '-' + str(time.month) + '-' + str(time.day) + \"_new\"))\n",
    "                        \n",
    "    time += dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Dr. Uddin's A1 Solution, stemming has been added\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# Use regex to remove non-alpha characters\n",
    "def removeNonAlpha(word):\n",
    "    return regex.sub('', word)\n",
    " \n",
    "# Remove any special characters\n",
    "def remove_specialchars(word):\n",
    "    if word is None or word == \"\":\n",
    "        return word\n",
    "    exclude = set(string.punctuation)    \n",
    "    exclude.add('..')\n",
    "    exclude.add('*')\n",
    "    for c in exclude:\n",
    "        word = word.strip(c)\n",
    "    return word\n",
    "\n",
    "# Use snowball stemmer to stem the words\n",
    "def stem(word):\n",
    "    return snowball.stem(word)\n",
    "\n",
    "def default_zero():\n",
    "    return 0\n",
    "\n",
    "# pass the sentences through this function to preprocess the text\n",
    "def preprocess_text(text_input):\n",
    "  \n",
    "    \n",
    "    words = []\n",
    "    ticker_dict = defaultdict(default_zero)\n",
    "    \n",
    "    # Tokenize into sentences, then into words\n",
    "    sents = nltk.sent_tokenize(text_input)\n",
    "    for sent in sents:\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            \n",
    "            # Remove stop words, specials chars, non-alpha, and stem the words\n",
    "            if word.lower() in stop_words: continue\n",
    "            word = remove_specialchars(word)\n",
    "\n",
    "            # Check if any of the words corresponds to a stock ticker\n",
    "            # Case sensitive at this point (assuming tickers are all caps)\n",
    "            # Make the words all lowercase (unless ticker)\n",
    "            if word in tickers:\n",
    "                ticker_dict[word] += 1\n",
    "            else:\n",
    "                word = word.lower()\n",
    "  \n",
    "            word = removeNonAlpha(word)\n",
    "#             word = stem(word)\n",
    "            \n",
    "            # Remove words with length shorter than 3 characters if it's not a ticker\n",
    "            if len(word) < 3 and word not in ticker_dict.keys(): continue\n",
    "            \n",
    "            if word is not None:\n",
    "                words.append(word)\n",
    "                \n",
    "        \n",
    "    # Return the preprocessed text\n",
    "    return \" \".join(words), ticker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.datetime(2020, 11, 24)\n",
    "\n",
    "# Drop rows that don't include ticker in title or body\n",
    "while (time <= dt.datetime(2022, 11, 15)):\n",
    "    try:\n",
    "        df = pd.read_pickle(\"./pickles/new score by day/{}_new.pkl\".format(\n",
    "            str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n",
    "    except FileNotFoundError:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "        \n",
    "    if df.empty:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    # Create a new column that combines title and body\n",
    "    df['fulltext'] = df['title'] + \" \" + df['selftext']\n",
    "    df.dropna(subset=['fulltext'])\n",
    "    # Preprocess the title and body and determine if either contains a stock ticker\n",
    "    df['fulltext_processed'], df['ticker'] = zip(*df['fulltext'].apply(preprocess_text))\n",
    "\n",
    "    # Remove posts that don't contain a ticker in the title or body\n",
    "    df = df[df['ticker'] != {}]     \n",
    "                        \n",
    "    df.to_pickle(\"./pickles/has ticker/{}.pkl\".format(\n",
    "        str(time.year) + '-' + str(time.month) + '-' + str(time.day) + \"_ticker\"))\n",
    "                        \n",
    "    time += dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b397620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.datetime(2019,1,1)\n",
    "\n",
    "while (time <= dt.datetime(2022, 11, 15)):\n",
    "    try:\n",
    "        df = pd.read_pickle(\"./pickles/has ticker/{}_ticker.pkl\".format(\n",
    "            str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n",
    "    except FileNotFoundError:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "        \n",
    "    if df.empty:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "    \n",
    "    #get the Highest Ticker\n",
    "    list_max = []\n",
    "    ticker_dictionary_list = df['ticker'].tolist()\n",
    "    for i in ticker_dictionary_list:\n",
    "        list_max.append(max(i, key= i.get))\n",
    "        \n",
    "    df['ticker'] = list_max                \n",
    "    df.to_pickle(\"./pickles/Main ticker/{}.pkl\".format(\n",
    "        str(time.year) + '-' + str(time.month) + '-' + str(time.day) + \"_main_ticker\"))\n",
    "                        \n",
    "    time += dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e11577",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.datetime(2019, 1, 1)\n",
    "full_dataset = pd.DataFrame()\n",
    "\n",
    "# Consolidate our data into one big dataframe\n",
    "while (time <= dt.datetime(2022, 11, 14)):\n",
    "    try:\n",
    "        df = pd.read_pickle(\"./pickles/Main ticker/{}_main_ticker.pkl\".format(\n",
    "            str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n",
    "    except FileNotFoundError:\n",
    "        time += dt.timedelta(days=1)\n",
    "        continue\n",
    "        \n",
    "    full_dataset = pd.concat([full_dataset, df], ignore_index=True)\n",
    "    \n",
    "    time += dt.timedelta(days=1)\n",
    "\n",
    "full_dataset.to_csv('full_dataset_main.csv')\n",
    "full_dataset.to_pickle('full_dataset_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f4f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d8559e58fad08f6b149f9f678acd42e602cfd8cee785110be98faa6bcb4ac45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
