{"cells":[{"cell_type":"markdown","source":["# Data Collection"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20bdd3ca-c5e2-4003-a629-63eff7e9d8df","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Importing Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c5541db-cbc4-4ebd-936b-95707966036c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!pip install praw\n!pip install nltk\nimport json\nimport pandas as pd\nimport requests\nimport datetime as dt\nimport praw\nfrom datetime import timedelta \nimport nltk \nfrom collections import defaultdict \nfrom nltk.stem import SnowballStemmer\nsnowball = SnowballStemmer(language = \"english\")\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\nimport string\nnltk.download('words')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\nimport regex as re"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa7ff10f-8e2c-443f-b4ef-6da57dc05bb9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Requirement already satisfied: praw in /databricks/python3/lib/python3.8/site-packages (7.6.1)\r\nRequirement already satisfied: update-checker>=0.18 in /databricks/python3/lib/python3.8/site-packages (from praw) (0.18.0)\r\nRequirement already satisfied: websocket-client>=0.54.0 in /databricks/python3/lib/python3.8/site-packages (from praw) (1.4.2)\r\nRequirement already satisfied: prawcore<3,>=2.1 in /databricks/python3/lib/python3.8/site-packages (from praw) (2.3.0)\r\nRequirement already satisfied: requests<3.0,>=2.6.0 in /databricks/python3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.25.11)\r\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.8)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.1.3)\r\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2022.10.31)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.64.1)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: praw in /databricks/python3/lib/python3.8/site-packages (7.6.1)\r\nRequirement already satisfied: update-checker>=0.18 in /databricks/python3/lib/python3.8/site-packages (from praw) (0.18.0)\r\nRequirement already satisfied: websocket-client>=0.54.0 in /databricks/python3/lib/python3.8/site-packages (from praw) (1.4.2)\r\nRequirement already satisfied: prawcore<3,>=2.1 in /databricks/python3/lib/python3.8/site-packages (from praw) (2.3.0)\r\nRequirement already satisfied: requests<3.0,>=2.6.0 in /databricks/python3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.25.11)\r\nRequirement already satisfied: idna<3,>=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.8)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.1.3)\r\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2022.10.31)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.64.1)\r\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Get the list of tickers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a07329e9-3911-43a2-b88a-6a56eedb2bb4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\n\ntickers = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/ferasdahrooge@gmail.com/listoftickers-2.csv\")\ntickers = tickers.select(\"Symbol\")\ntickers= tickers.rdd.map(lambda x: x.Symbol).collect()\ntickers.remove(\"OC\")\ntickers.remove(\"DD\")\ntickers.remove(\"TA\")\n\n# Remove single character tickers\ntickers = [str(x) for x in tickers if len(str(x)) != 1]\n\ntickers.append(\"SPY\")\ntickers.append(\"V\") #Visa\ntickers.append(\"T\") #AT&T\ntickers.append(\"C\") #Citigroup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2387aef-deaa-4808-9495-ad50ada1d488","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## CollectData From Pushshift API (DataCollection Part 1)\n\n### Coded by: Feras and Ardit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a8b30be-44fe-46e0-b351-c7f65dd4a4de","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["d = {}\nend_time = dt.datetime(2022, 11, 15)\nstart_time = dt.datetime(2022,11, 14)\nurl = 'https://api.pushshift.io/reddit/search/submission'\ndef collectData(subreddit, size):\n    after = int(start_time.timestamp())\n    before = int(end_time.timestamp())\n    for i in range(1, 100):\n        if i==1:\n            params = {\"subreddit\":subreddit, 'size':1000, 'before': before, 'after': after}\n        else:\n            params = {\"subreddit\":subreddit, 'size':size,'before': before, 'after': after}\n        res = requests.get(url, params)    \n        data = res.json()\n        dat = data['data']\n        before = dat[-1]['created_utc']\n        df_name = subreddit+str(i)\n        d[df_name] = pd.DataFrame(dat)\n        return d"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d69dbb0c-8054-4a5b-84e3-6ea413149b0c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Build DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8d86ccc-352b-4309-b527-834f1ad96822","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def buildDataFrame():\n    dataFrame = collectData('wallstreetbets', 1000)\n    df = dataFrame[\"wallstreetbets1\"]\n    df['Time of Creation'] = df['created_utc'].map(lambda t: dt.datetime.fromtimestamp(t))\n    df = df[['id', 'title', 'selftext', 'author', 'score', 'num_comments','Time of Creation']]\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2266539c-fb2e-4bd4-878c-f4915f04795d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Save into Pickle File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"583fb3bb-b175-4c6b-a922-1cf12c79bee0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\nwhile (end_time <= dt.datetime(2022, 11,15)):\n    data = buildDataFrame()\n    data = spark.createDataFrame(data) \n    data.rdd.saveAsPickleFile(\"FileStore/output/old score by day/{}.pkl\".format(str(start_time.year) + '-' + str(start_time.month) + '-' + str(start_time.day)))\n    start_time += dt.timedelta(days = 1)\n    end_time += dt.timedelta(days = 1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99c61114-71a5-4c15-a931-17ea4f6df621","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2038261933220532>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuildDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"FileStore/output/old score by day/{}.pkl\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmonth\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mday\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mstart_time\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mend_time\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsPickleFile\u001B[0;34m(self, path, batchSize)\u001B[0m\n\u001B[1;32m   1800\u001B[0m             \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatchSize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1801\u001B[0m         \u001B[0mreserialized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reserialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1802\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreserialized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1803\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1804\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsPickleFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/old score by day/2022-11-14.pkl already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1610)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1610)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:575)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:574)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsPickleFile(PythonRDD.scala:845)\n\tat org.apache.spark.api.python.PythonRDD.saveAsPickleFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor779.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/old score by day/2022-11-14.pkl already exists","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2038261933220532>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuildDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"FileStore/output/old score by day/{}.pkl\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmonth\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mday\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mstart_time\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mend_time\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsPickleFile\u001B[0;34m(self, path, batchSize)\u001B[0m\n\u001B[1;32m   1800\u001B[0m             \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatchSize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1801\u001B[0m         \u001B[0mreserialized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reserialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1802\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreserialized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1803\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1804\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsPickleFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/old score by day/2022-11-14.pkl already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1610)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1610)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:575)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:574)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsPickleFile(PythonRDD.scala:845)\n\tat org.apache.spark.api.python.PythonRDD.saveAsPickleFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor779.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### example of data output for DataCollection Part 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6dc129d9-cf40-4c62-b81e-a3cf828e373d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"537a6777-0721-4a0f-a456-577f68931f17","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------+--------------------+---------+------------------+-----+------------+-------------------+\n|    id|               title| selftext|            author|score|num_comments|   Time of Creation|\n+------+--------------------+---------+------------------+-----+------------+-------------------+\n|yvgxje|The only way to e...|         |     WebbsPowerade|    1|           1|2022-11-14 23:59:22|\n|yvgw60|I choose to belie...|         | mercilessfatehate|    1|           0|2022-11-14 23:57:35|\n|yvgvph|Jim Cramer is a C...|         |        Ciasecrets|    1|           0|2022-11-14 23:57:01|\n|yvgopq|An inverted yield...|         |        56Kdial_up|    1|           2|2022-11-14 23:48:31|\n|yvghmj|Can you bring bac...|[removed]|        WisedKanny|    1|           0|2022-11-14 23:39:59|\n|yvghk1|Definitely pulled...|         |  Nearby-Soil-5668|    1|           2|2022-11-14 23:39:52|\n|yvg65f|Sell before earni...|[removed]|        FrankieH0T|    1|           1|2022-11-14 23:26:05|\n|yvg5w3|         Day trading|[removed]|HearingHelpful2878|    1|           0|2022-11-14 23:25:46|\n|yvg3pe|         Day trading|[removed]|HearingHelpful2878|    1|           1|2022-11-14 23:23:15|\n|yvg1zm|So I sell my call...|         |        FrankieH0T|    1|           1|2022-11-14 23:21:09|\n|yvfzhm|Perfect screensho...|         |        omenoflord|    1|           2|2022-11-14 23:18:25|\n|yvfwyd|Do I keep or sell...|         |        FrankieH0T|    1|           2|2022-11-14 23:15:25|\n|yvfwas|      Recommendation|[removed]|    We-ll-carry-on|    1|           0|2022-11-14 23:14:45|\n|yvfu80|CEO of AMC Adam A...|         |           natbibi|    1|           2|2022-11-14 23:12:19|\n|yvfnp0|day trading with ...|[removed]|BusinessCookie2032|    1|           0|2022-11-14 23:05:10|\n|yvfjj2| Calls on Blue Apron|         |      xjackstonerx|    1|           0|2022-11-14 23:00:24|\n|yvfiat|Looks like it’s s...|         |        gundawg300|    1|           1|2022-11-14 22:59:20|\n|yvfglt|Visual Mod has be...|         | Anxious-Lake-1160|    1|           2|2022-11-14 22:57:26|\n|yvfe6f|Visual Mod is man...|[removed]| Anxious-Lake-1160|    1|           0|2022-11-14 22:54:54|\n|yvfbkk|How Sam Bankman-F...|         |      PNWtreeguy69|    1|           0|2022-11-14 22:51:49|\n+------+--------------------+---------+------------------+-----+------------+-------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------------------+---------+------------------+-----+------------+-------------------+\n|    id|               title| selftext|            author|score|num_comments|   Time of Creation|\n+------+--------------------+---------+------------------+-----+------------+-------------------+\n|yvgxje|The only way to e...|         |     WebbsPowerade|    1|           1|2022-11-14 23:59:22|\n|yvgw60|I choose to belie...|         | mercilessfatehate|    1|           0|2022-11-14 23:57:35|\n|yvgvph|Jim Cramer is a C...|         |        Ciasecrets|    1|           0|2022-11-14 23:57:01|\n|yvgopq|An inverted yield...|         |        56Kdial_up|    1|           2|2022-11-14 23:48:31|\n|yvghmj|Can you bring bac...|[removed]|        WisedKanny|    1|           0|2022-11-14 23:39:59|\n|yvghk1|Definitely pulled...|         |  Nearby-Soil-5668|    1|           2|2022-11-14 23:39:52|\n|yvg65f|Sell before earni...|[removed]|        FrankieH0T|    1|           1|2022-11-14 23:26:05|\n|yvg5w3|         Day trading|[removed]|HearingHelpful2878|    1|           0|2022-11-14 23:25:46|\n|yvg3pe|         Day trading|[removed]|HearingHelpful2878|    1|           1|2022-11-14 23:23:15|\n|yvg1zm|So I sell my call...|         |        FrankieH0T|    1|           1|2022-11-14 23:21:09|\n|yvfzhm|Perfect screensho...|         |        omenoflord|    1|           2|2022-11-14 23:18:25|\n|yvfwyd|Do I keep or sell...|         |        FrankieH0T|    1|           2|2022-11-14 23:15:25|\n|yvfwas|      Recommendation|[removed]|    We-ll-carry-on|    1|           0|2022-11-14 23:14:45|\n|yvfu80|CEO of AMC Adam A...|         |           natbibi|    1|           2|2022-11-14 23:12:19|\n|yvfnp0|day trading with ...|[removed]|BusinessCookie2032|    1|           0|2022-11-14 23:05:10|\n|yvfjj2| Calls on Blue Apron|         |      xjackstonerx|    1|           0|2022-11-14 23:00:24|\n|yvfiat|Looks like it’s s...|         |        gundawg300|    1|           1|2022-11-14 22:59:20|\n|yvfglt|Visual Mod has be...|         | Anxious-Lake-1160|    1|           2|2022-11-14 22:57:26|\n|yvfe6f|Visual Mod is man...|[removed]| Anxious-Lake-1160|    1|           0|2022-11-14 22:54:54|\n|yvfbkk|How Sam Bankman-F...|         |      PNWtreeguy69|    1|           0|2022-11-14 22:51:49|\n+------+--------------------+---------+------------------+-----+------------+-------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Fix Score Using Python Reddit API Wrapper (DataCollection Part 2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4eeaf3b-6fce-4603-ad19-3e5601ac3a0a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Use Praw to Extract the Right Score and Remove Bot Posts with scores over 100"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a85c87fd-fc20-41d5-ae8c-17b83c8085b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def reddit_score_fix(df, client_id, client_secret, user_agent, password, username):\n    df = df.toPandas()\n    reddit = praw.Reddit(\n        client_id=client_id,\n        client_secret=client_secret,\n        password=password,\n        user_agent=user_agent,\n        username=username,\n    )\n    min_score = 100\n    df['new_id'] = \"t3_\" + df[\"id\"]\n    gentest = reddit.info(fullnames = df['new_id'].to_list())\n    midlist = [[item.id, item.score] for item in gentest]\n    df_midlist = pd.DataFrame(midlist, columns = ['ID', 'SCORE'])\n    df_merged = pd.merge(df, df_midlist, left_on = 'id', right_on = 'ID', how = 'outer')\n    df['score'] = df_merged['SCORE']\n\n    df = df[df['score'] > min_score]\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"898111d6-c88a-4fb6-896e-8d38826be804","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Save into Pickle File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fbf0182-e4cd-43f1-91d9-dec1ea7d9738","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["time = dt.datetime(2022, 11, 15)\n\nwhile (time <= dt.datetime(2022, 11, 15)):\n    try:\n        pickleRdd = sc.pickleFile(\"FileStore/output/old score by day/{}.pkl\".format(str(time.year) + '-' + str(time.month) + '-' + str(time.day))).collect()\n        preFixed_score_data = spark.createDataFrame(pickleRdd)\n    except FileNotFoundError:\n        time += dt.timedelta(days=1)\n        continue\n    fixed_score_data = reddit_score_fix(preFixed_score_data, 'inJ6zJcKI7dSDOEnYtZtSg','HMVgC-yLRwpqsWAnYfPulFCna2ZbRA','ENSF612 - Pulling Score','mengsoftware','ensf612')\n    fixed_score_data = spark.createDataFrame(fixed_score_data) \n    fixed_score_data.rdd.saveAsPickleFile(\"FileStore/output/new score by day/{}.pkl\".format(str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n    \n    time += dt.timedelta(days=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cba92c5-4723-4408-87e8-12ef12e0127f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2038261933220537>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mfixed_score_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreddit_score_fix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreFixed_score_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inJ6zJcKI7dSDOEnYtZtSg'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'HMVgC-yLRwpqsWAnYfPulFCna2ZbRA'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'ENSF612 - Pulling Score'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'mengsoftware'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'ensf612'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mfixed_score_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfixed_score_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mfixed_score_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"FileStore/output/new score by day/{}.pkl\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmonth\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mday\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mtime\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsPickleFile\u001B[0;34m(self, path, batchSize)\u001B[0m\n\u001B[1;32m   1800\u001B[0m             \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatchSize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1801\u001B[0m         \u001B[0mreserialized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reserialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1802\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreserialized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1803\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1804\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsPickleFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/new score by day/2022-11-15.pkl already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1610)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1610)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:575)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:574)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsPickleFile(PythonRDD.scala:845)\n\tat org.apache.spark.api.python.PythonRDD.saveAsPickleFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor779.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/new score by day/2022-11-15.pkl already exists","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2038261933220537>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mfixed_score_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreddit_score_fix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreFixed_score_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inJ6zJcKI7dSDOEnYtZtSg'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'HMVgC-yLRwpqsWAnYfPulFCna2ZbRA'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'ENSF612 - Pulling Score'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'mengsoftware'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'ensf612'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mfixed_score_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfixed_score_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mfixed_score_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"FileStore/output/new score by day/{}.pkl\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0myear\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmonth\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mday\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mtime\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimedelta\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdays\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsPickleFile\u001B[0;34m(self, path, batchSize)\u001B[0m\n\u001B[1;32m   1800\u001B[0m             \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPickleSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatchSize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1801\u001B[0m         \u001B[0mreserialized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reserialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1802\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsPickleFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreserialized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1803\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1804\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodecClass\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsPickleFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/output/new score by day/2022-11-15.pkl already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1610)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1610)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:575)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:574)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsPickleFile(PythonRDD.scala:845)\n\tat org.apache.spark.api.python.PythonRDD.saveAsPickleFile(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor779.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### example of data output for DataCollection Part 2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"525a9a51-1cb9-46cc-a4b7-8c286882334a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["fixed_score_data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98f5e7f4-2b44-46eb-a3fb-f7c82e52a041","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\n|    id|               title|            selftext|              author|score|num_comments|   Time of Creation|   new_id|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\n|ywdnhw|bears: “AMZN will...|                    |  usayhesjustafriend|  189|           2|2022-11-15 23:51:53|t3_ywdnhw|\n|ywd13d|GEO Group Ape Tho...|                    |Financial-Process-86|  304|           0|2022-11-15 23:25:43|t3_ywd13d|\n|ywcxtu|MEME STOCK YOLO —...|                    |         Key-Ear7468|  288|           2|2022-11-15 23:21:53|t3_ywcxtu|\n|ywb5p1|     Welp this is it|\\-30k in June and...|         Tom-W-Davis| 2508|           1|2022-11-15 22:10:06|t3_ywb5p1|\n|ywaooz|Are we gonna see ...|                    |Which-Instruction-67|  332|           2|2022-11-15 21:50:36|t3_ywaooz|\n|ywaemz|Are you people fu...|                    |AlternativeTurnip307| 3722|           1|2022-11-15 21:39:25|t3_ywaemz|\n|ywa2u8|      my 🌈🐻 copium|                    |       Spare-Help562|  357|           1|2022-11-15 21:26:29|t3_ywa2u8|\n|yw9e7t|What Are Your Mov...|**Watch WallStree...|OPINION_IS_UNPOPULAR|  171|           7|2022-11-15 21:00:12|t3_yw9e7t|\n|yw8pz6|Looks like it’s s...|                    |          gundawg300|  392|           1|2022-11-15 20:33:37|t3_yw8pz6|\n|yw8f89|Michael J Burry j...|                    |        RocketBoomGo|  864|           0|2022-11-15 20:21:52|t3_yw8f89|\n|yw824c|Directed by Marti...|                    |           Outof_ITM|  392|           0|2022-11-15 20:07:35|t3_yw824c|\n|yw75eb|Today we see the ...|                    |      soareyousaying|  230|           1|2022-11-15 19:31:46|t3_yw75eb|\n|yw72zc|Two missiles have...|so that rumor spa...|          Wishful10x|  408|           2|2022-11-15 19:29:18|t3_yw72zc|\n|yw6ryb| 1 Million in 1 Year|                    |            aquaBluu| 4700|           2|2022-11-15 19:17:13|t3_yw6ryb|\n|yw5vlm|Russia messed up....|                    |      ParticleEngine|  217|           2|2022-11-15 18:42:28|t3_yw5vlm|\n|yw5kwn|           well boys|                    |              Jiuk_y|26077|           1|2022-11-15 18:30:41|t3_yw5kwn|\n|yw5ge9|Put my lifesaving...|                    |    Away_Fishing5926| 4179|           2|2022-11-15 18:25:46|t3_yw5ge9|\n|yw5763|How will NVIDIA c...|&amp;#x200B;\\n\\nh...|          ShopBitter| 4981|           1|2022-11-15 18:15:31|t3_yw5763|\n|yw3zjl|$28k —&gt; $303k ...|                    |          atrain1189|  992|           2|2022-11-15 17:33:08|t3_yw3zjl|\n|yw3xez|SPX is about to t...|                    |             krakdis|  489|           2|2022-11-15 17:31:10|t3_yw3xez|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\n|    id|               title|            selftext|              author|score|num_comments|   Time of Creation|   new_id|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\n|ywdnhw|bears: “AMZN will...|                    |  usayhesjustafriend|  189|           2|2022-11-15 23:51:53|t3_ywdnhw|\n|ywd13d|GEO Group Ape Tho...|                    |Financial-Process-86|  304|           0|2022-11-15 23:25:43|t3_ywd13d|\n|ywcxtu|MEME STOCK YOLO —...|                    |         Key-Ear7468|  288|           2|2022-11-15 23:21:53|t3_ywcxtu|\n|ywb5p1|     Welp this is it|\\-30k in June and...|         Tom-W-Davis| 2508|           1|2022-11-15 22:10:06|t3_ywb5p1|\n|ywaooz|Are we gonna see ...|                    |Which-Instruction-67|  332|           2|2022-11-15 21:50:36|t3_ywaooz|\n|ywaemz|Are you people fu...|                    |AlternativeTurnip307| 3722|           1|2022-11-15 21:39:25|t3_ywaemz|\n|ywa2u8|      my 🌈🐻 copium|                    |       Spare-Help562|  357|           1|2022-11-15 21:26:29|t3_ywa2u8|\n|yw9e7t|What Are Your Mov...|**Watch WallStree...|OPINION_IS_UNPOPULAR|  171|           7|2022-11-15 21:00:12|t3_yw9e7t|\n|yw8pz6|Looks like it’s s...|                    |          gundawg300|  392|           1|2022-11-15 20:33:37|t3_yw8pz6|\n|yw8f89|Michael J Burry j...|                    |        RocketBoomGo|  864|           0|2022-11-15 20:21:52|t3_yw8f89|\n|yw824c|Directed by Marti...|                    |           Outof_ITM|  392|           0|2022-11-15 20:07:35|t3_yw824c|\n|yw75eb|Today we see the ...|                    |      soareyousaying|  230|           1|2022-11-15 19:31:46|t3_yw75eb|\n|yw72zc|Two missiles have...|so that rumor spa...|          Wishful10x|  408|           2|2022-11-15 19:29:18|t3_yw72zc|\n|yw6ryb| 1 Million in 1 Year|                    |            aquaBluu| 4700|           2|2022-11-15 19:17:13|t3_yw6ryb|\n|yw5vlm|Russia messed up....|                    |      ParticleEngine|  217|           2|2022-11-15 18:42:28|t3_yw5vlm|\n|yw5kwn|           well boys|                    |              Jiuk_y|26077|           1|2022-11-15 18:30:41|t3_yw5kwn|\n|yw5ge9|Put my lifesaving...|                    |    Away_Fishing5926| 4179|           2|2022-11-15 18:25:46|t3_yw5ge9|\n|yw5763|How will NVIDIA c...|&amp;#x200B;\\n\\nh...|          ShopBitter| 4981|           1|2022-11-15 18:15:31|t3_yw5763|\n|yw3zjl|$28k —&gt; $303k ...|                    |          atrain1189|  992|           2|2022-11-15 17:33:08|t3_yw3zjl|\n|yw3xez|SPX is about to t...|                    |             krakdis|  489|           2|2022-11-15 17:31:10|t3_yw3xez|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Including Ticker (DataCollection Part 3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c536b9b-9655-4b78-8124-febcae9c6da3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Preprocess the DataCollected"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b738f726-cf3b-47bc-bf6d-90a4304a09a8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Based on Dr. Uddin's A1 Solution, stemming has been added\n\nregex = re.compile('[^a-zA-Z]')\n\n# Use regex to remove non-alpha characters\ndef removeNonAlpha(word):\n    return regex.sub('', word)\n \ndef remove_specialchars(word):\n    if word is None or word == \"\":\n        return word\n    exclude = set(string.punctuation)    \n    exclude.add('..')\n    exclude.add('*')\n    for c in exclude:\n        word = word.strip(c)\n    return word\n\n# Use snowball stemmer to stem the words\ndef stem(word):\n    return snowball.stem(word)\n\ndef default_zero():\n    return 0\n\n# pass the sentences through this function to preprocess the text\ndef preprocess_text(text_input):\n    text_input = str(text_input)\n  \n    words = []\n    ticker_dict = defaultdict(default_zero)\n    \n    # Tokenize into sentences, then into words\n    sents = nltk.sent_tokenize(text_input)\n    for sent in sents:\n        for word in nltk.word_tokenize(sent):\n            \n            # Remove stop words, specials chars, non-alpha, and stem the words\n            if word.lower() in stop_words: continue\n            word = remove_specialchars(word)\n\n            # Check if any of the words corresponds to a stock ticker\n            # Case sensitive at this point (assuming tickers are all caps)\n            # Make the words all lowercase (unless ticker)\n            if word in tickers:\n                ticker_dict[word] += 1\n            else:\n                word = word.lower()\n  \n            word = removeNonAlpha(word)\n#             word = stem(word)\n            \n            # Remove words with length shorter than 3 characters if it's not a ticker\n            if len(word) < 3 and word not in ticker_dict.keys(): continue\n            \n            if word is not None:\n                words.append(word)\n                \n        \n    # Return the preprocessed text\n    return \" \".join(words), ticker_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88a2d164-31ec-4da8-b0cb-57026954f5cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Include the rows that only contains tickers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc7c7674-8955-4352-a709-c5269f9b9f37","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def dataFrame_including_ticker(df):\n    df['fulltext'] = df['title'] + \" \" + df['selftext']\n    # Preprocess the title and body and determine if either contains a stock ticker\n    df['fulltext_processed'], df['ticker'] = zip(*df['fulltext'].apply(preprocess_text))\n    df.dropna(subset=['fulltext'])\n    # Remove posts that don't contain a ticker in the title or body\n    df = df[df['ticker'] != {}] \n    list_max = []\n    ticker_dictionary_list = df['ticker'].tolist()\n    for i in ticker_dictionary_list:\n        list_max.append(max(i, key= i.get))\n        \n    df['ticker'] = list_max                \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ef47433-26e4-40d7-81cc-f03d80a4bade","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Save them into a Pickle File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8417392-0470-4346-a598-6eaa26a3d400","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["time = dt.datetime(2022,11,14)\n\nwhile (time <= dt.datetime(2022, 11, 14)):\n    try:\n        pickleRdd = sc.pickleFile(\"FileStore/output/new score by day/{}.pkl\".format(str(time.year) + '-' + str(time.month) + '-' + str(time.day))).collect()\n        fixed_score_data = spark.createDataFrame(pickleRdd)\n    except FileNotFoundError:\n        time += dt.timedelta(days=1)\n        continue\n        \n    fixed_score_data = fixed_score_data.toPandas()    \n    if fixed_score_data.empty:\n        time += dt.timedelta(days=1)\n        continue\n        \n    fixed_has_ticker_dataFrame = dataFrame_including_ticker(fixed_score_data)\n    fixed_has_ticker_dataFrame = spark.createDataFrame(fixed_has_ticker_dataFrame) \n    fixed_has_ticker_dataFrame.rdd.saveAsPickleFile(\"FileStore/output/main ticker/{}.pkl\".format(str(time.year) + '-' + str(time.month) + '-' + str(time.day)))\n    time += dt.timedelta(days=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f73fa8e6-8049-486a-a6a0-5054bcbae5cb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"<command-2038261933220543>:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ticker'] = list_max\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<command-2038261933220543>:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ticker'] = list_max\n"]}}],"execution_count":0},{"cell_type":"code","source":["fixed_has_ticker_dataFrame.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"495eae9c-1e39-478a-8df3-9767e56efb43","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n|    id|               title|            selftext|              author|score|num_comments|   Time of Creation|   new_id|            fulltext|  fulltext_processed|ticker|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n|yvdlow|Now short on~ max...|Just wanted to sa...|MikasalsTheBestWaifu|  106|           2|2022-11-14 21:43:47|t3_yvdlow|Now short on~ max...|short max margin ...|  ASTS|\n|yvcgp5|What Are Your Mov...|**Watch WallStree...|OPINION_IS_UNPOPULAR|  165|           3|2022-11-14 21:00:11|t3_yvcgp5|What Are Your Mov...|moves tomorrow no...|    DM|\n|yv839x|Amazon reportedly...|Amazon is plannin...|       predictany007|  105|           1|2022-11-14 18:27:08|t3_yv839x|Amazon reportedly...|amazon reportedly...|  AMZN|\n|yv311s|YOLO’d $185k in G...|                    |          atrain1189|  210|           2|2022-11-14 15:39:19|t3_yv311s|YOLO’d $185k in G...|yolo GOOG calls n...|  GOOG|\n|yuz5jh|I AM OFFICIALLY A...|The unfurling of ...|MikasalsTheBestWaifu|19700|           1|2022-11-14 13:18:57|t3_yuz5jh|I AM OFFICIALLY A...|officially fking ...|  ASTS|\n|yuxn7l|ASTS confirmed un...|                    |Aggravating-Curve755| 1245|           1|2022-11-14 12:18:35|t3_yuxn7l|ASTS confirmed un...|ASTS confirmed un...|  ASTS|\n|yuwrzz|How will SPY clos...| \\n\\nhttps://prev...|          ShopBitter| 5571|           1|2022-11-14 11:40:53|t3_yuwrzz|How will SPY clos...|SPY close tuesday...|   SPY|\n|yupb8g|My Full Plan for ...|Hello ladies and ...|  Flimsy-Willow-3086|  123|           0|2022-11-14 04:55:34|t3_yupb8g|My Full Plan for ...|full plan week ch...|    UK|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n|    id|               title|            selftext|              author|score|num_comments|   Time of Creation|   new_id|            fulltext|  fulltext_processed|ticker|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n|yvdlow|Now short on~ max...|Just wanted to sa...|MikasalsTheBestWaifu|  106|           2|2022-11-14 21:43:47|t3_yvdlow|Now short on~ max...|short max margin ...|  ASTS|\n|yvcgp5|What Are Your Mov...|**Watch WallStree...|OPINION_IS_UNPOPULAR|  165|           3|2022-11-14 21:00:11|t3_yvcgp5|What Are Your Mov...|moves tomorrow no...|    DM|\n|yv839x|Amazon reportedly...|Amazon is plannin...|       predictany007|  105|           1|2022-11-14 18:27:08|t3_yv839x|Amazon reportedly...|amazon reportedly...|  AMZN|\n|yv311s|YOLO’d $185k in G...|                    |          atrain1189|  210|           2|2022-11-14 15:39:19|t3_yv311s|YOLO’d $185k in G...|yolo GOOG calls n...|  GOOG|\n|yuz5jh|I AM OFFICIALLY A...|The unfurling of ...|MikasalsTheBestWaifu|19700|           1|2022-11-14 13:18:57|t3_yuz5jh|I AM OFFICIALLY A...|officially fking ...|  ASTS|\n|yuxn7l|ASTS confirmed un...|                    |Aggravating-Curve755| 1245|           1|2022-11-14 12:18:35|t3_yuxn7l|ASTS confirmed un...|ASTS confirmed un...|  ASTS|\n|yuwrzz|How will SPY clos...| \\n\\nhttps://prev...|          ShopBitter| 5571|           1|2022-11-14 11:40:53|t3_yuwrzz|How will SPY clos...|SPY close tuesday...|   SPY|\n|yupb8g|My Full Plan for ...|Hello ladies and ...|  Flimsy-Willow-3086|  123|           0|2022-11-14 04:55:34|t3_yupb8g|My Full Plan for ...|full plan week ch...|    UK|\n+------+--------------------+--------------------+--------------------+-----+------------+-------------------+---------+--------------------+--------------------+------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Collect the data into one CSV File"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45e4e8c0-9b39-499f-b3ad-8464a19a5fcc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["time = dt.datetime(2022, 11, 14)\nfull_dataset = pd.DataFrame()\n\n# Consolidate our data into one big dataframe\nwhile (time <= dt.datetime(2022, 11, 14)):\n    try:\n        pickleRdd = sc.pickleFile(\"FileStore/output/main ticker/{}.pkl\".format(str(time.year) + '-' + str(time.month) + '-' + str(time.day))).collect()\n        main_ticker_dataFrame = spark.createDataFrame(pickleRdd)\n    except FileNotFoundError:\n        time += dt.timedelta(days=1)\n        continue\n    main_ticker_dataFrame = main_ticker_dataFrame.toPandas()\n    full_dataset = pd.concat([full_dataset, main_ticker_dataFrame], ignore_index=True)\n    \n    time += dt.timedelta(days=1)\n\nfull_dataset = spark.createDataFrame(full_dataset)  \nfull_dataset.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").option(\"compression\", \"gzip\").save(\"dbfs:/FileStore/df/full_dataset.csv\")\nfull_dataset.rdd.saveAsPickleFile(\"FileStore/output/full_dataset.pkl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25509c3f-c905-45e0-8ee3-bb73d9379a03","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Datacollection-Wallstreet","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4060027095787529}},"nbformat":4,"nbformat_minor":0}
